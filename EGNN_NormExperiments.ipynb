{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db9c622a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(5000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 5 seconds\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "sys.path.append(\".\")  \n",
    "\n",
    "EXP_NAME = \"BN\"\n",
    "\n",
    "nblog = open(\"{}.log\".format(EXP_NAME), \"a+\")\n",
    "sys.stdout.echo = nblog\n",
    "sys.stderr.echo = nblog\n",
    "\n",
    "get_ipython().log.handlers[0].stream = nblog\n",
    "get_ipython().log.setLevel(logging.INFO)\n",
    "\n",
    "%autosave 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93ab8d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.gcl import unsorted_segment_sum, E_GCL\n",
    "from qm9 import utils as qm9_utils\n",
    "from qm9 import dataset\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5928a2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGNN_BN(nn.Module):\n",
    "    def __init__(self, num_features, x_dim, momentum=0.9, eps=1e-5, pos=False):\n",
    "        super().__init__()\n",
    "\n",
    "        h_shape = (1, num_features)\n",
    "        x_shape = (1, x_dim)\n",
    "\n",
    "        self.gamma_h = nn.Parameter(torch.ones(h_shape))\n",
    "        self.beta_h = nn.Parameter(torch.zeros(h_shape))\n",
    "\n",
    "        # self.gamma_x = nn.Parameter(torch.ones(x_shape))\n",
    "        # self.beta_x = nn.Parameter(torch.zeros(x_shape))\n",
    "\n",
    "        self.pos = pos\n",
    "\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "\n",
    "        self.register_buffer('moving_mean_h', torch.ones(h_shape))\n",
    "        self.register_buffer('moving_var_h', torch.ones(h_shape))\n",
    "        # self.register_buffer('moving_mean_x', torch.ones(x_shape))\n",
    "        # self.register_buffer('moving_var_x', torch.ones(x_shape))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # self.moving_var_x.fill_(1)\n",
    "        self.moving_var_h.fill_(1)\n",
    "\n",
    "    def forward(self, h, x):\n",
    "        if self.training:\n",
    "            var_h, mean_h = torch.var_mean(h, dim=0, keepdim=True, unbiased=False)\n",
    "            # var_x, mean_x = torch.var_mean(x, dim=0, keepdim=True, unbiased=False)\n",
    "\n",
    "            self.moving_mean_h.mul_(self.momentum)\n",
    "            self.moving_mean_h.add_((1 - self.momentum) * mean_h)\n",
    "            self.moving_var_h.mul_(self.momentum)\n",
    "            self.moving_var_h.add_((1 - self.momentum) * var_h)\n",
    "\n",
    "            # self.moving_mean_x.mul_(self.momentum)\n",
    "            # self.moving_mean_x.add_((1 - self.momentum) * mean_x)\n",
    "            # self.moving_var_x.mul_(self.momentum)\n",
    "            # self.moving_var_x.add_((1 - self.momentum) * var_x)\n",
    "        else:\n",
    "            # var_x = self.moving_var_x\n",
    "            # mean_x = self.moving_mean_x\n",
    "            var_h = self.moving_var_h\n",
    "            mean_h = self.moving_mean_h\n",
    "            \n",
    "\n",
    "        h = (h - mean_h) * torch.rsqrt(var_h+self.eps)\n",
    "        # x = (x - mean_x) * torch.rsqrt(var_x+self.eps)\n",
    "\n",
    "        out_h = h * self.gamma_h + self.beta_h\n",
    "        # out_x = x * self.gamma_x + self.beta_x\n",
    "\n",
    "        # return out_h, out_x\n",
    "        return out_h\n",
    "\n",
    "class E_GCL_mask(E_GCL):\n",
    "    \"\"\"Graph Neural Net with global state and fixed number of nodes per graph.\n",
    "    Args:\n",
    "          hidden_dim: Number of hidden units.\n",
    "          num_nodes: Maximum number of nodes (for self-attentive pooling).\n",
    "          global_agg: Global aggregation function ('attn' or 'sum').\n",
    "          temp: Softmax temperature.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_nf, output_nf, hidden_nf, edges_in_d=0, nodes_attr_dim=0, act_fn=nn.ReLU(), recurrent=True, coords_weight=1.0, norm_diff=False, attention=False):\n",
    "        E_GCL.__init__(self, input_nf, output_nf, hidden_nf, edges_in_d=edges_in_d, nodes_att_dim=nodes_attr_dim, act_fn=act_fn, recurrent=recurrent, coords_weight=coords_weight, norm_diff=norm_diff, attention=attention)\n",
    "\n",
    "        del self.coord_mlp\n",
    "        self.act_fn = act_fn\n",
    "\n",
    "    def coord_model(self, coord, edge_index, coord_diff, edge_feat, edge_mask):\n",
    "        row, col = edge_index\n",
    "        trans = coord_diff * self.coord_mlp(edge_feat) * edge_mask\n",
    "        agg = unsorted_segment_sum(trans, row, num_segments=coord.size(0))\n",
    "        coord += agg*self.coords_weight\n",
    "        return coord\n",
    "\n",
    "    def forward(self, h, edge_index, coord, node_mask, edge_mask, edge_attr=None, node_attr=None, n_nodes=None):\n",
    "        row, col = edge_index\n",
    "        radial, coord_diff = self.coord2radial(edge_index, coord)\n",
    "\n",
    "        edge_feat = self.edge_model(h[row], h[col], radial, edge_attr)\n",
    "\n",
    "        edge_feat = edge_feat * edge_mask\n",
    "\n",
    "        # TO DO: edge_feat = edge_feat * edge_mask\n",
    "\n",
    "        #coord = self.coord_model(coord, edge_index, coord_diff, edge_feat, edge_mask)\n",
    "        h, agg = self.node_model(h, edge_index, edge_feat, node_attr)\n",
    "\n",
    "        return h, coord, edge_attr\n",
    "\n",
    "class EGNN(nn.Module):\n",
    "    def __init__(self, in_node_nf, in_edge_nf, hidden_nf, device='cpu', act_fn=nn.SiLU(), n_layers=4, coords_weight=1.0, attention=False, node_attr=1, normalize=\"None\"):\n",
    "        super(EGNN, self).__init__()\n",
    "        self.hidden_nf = hidden_nf\n",
    "        self.device = device\n",
    "        self.n_layers = n_layers\n",
    "        self.normalize = normalize\n",
    "\n",
    "        ### Encoder\n",
    "        self.embedding = nn.Linear(in_node_nf, hidden_nf)\n",
    "        self.node_attr = node_attr\n",
    "        if node_attr:\n",
    "            n_node_attr = in_node_nf\n",
    "        else:\n",
    "            n_node_attr = 0\n",
    "\n",
    "        if normalize == \"egnn\" or normalize == \"both\":\n",
    "            norm_diff = True\n",
    "        else:\n",
    "            norm_diff = False\n",
    "\n",
    "        for i in range(0, n_layers):\n",
    "            self.add_module(\"gcl_%d\" % i, E_GCL_mask(self.hidden_nf, self.hidden_nf, self.hidden_nf, edges_in_d=in_edge_nf, nodes_attr_dim=n_node_attr, act_fn=act_fn, recurrent=True, coords_weight=coords_weight, norm_diff = norm_diff, attention=attention))\n",
    "            if normalize == \"batch\" or normalize == \"both\":\n",
    "                self.add_module(\"bn_%d\" % i, EGNN_BN(self.hidden_nf, 3, pos=True))\n",
    "\n",
    "        self.node_dec = nn.Sequential(nn.Linear(self.hidden_nf, self.hidden_nf),\n",
    "                                      act_fn,\n",
    "                                      nn.Linear(self.hidden_nf, self.hidden_nf))\n",
    "\n",
    "        self.graph_dec = nn.Sequential(nn.Linear(self.hidden_nf, self.hidden_nf),\n",
    "                                       act_fn,\n",
    "                                       nn.Linear(self.hidden_nf, 1))\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, h0, x, edges, edge_attr, node_mask, edge_mask, n_nodes):\n",
    "        h = self.embedding(h0)\n",
    "        for i in range(0, self.n_layers):\n",
    "            if self.node_attr:\n",
    "                h, _, _ = self._modules[\"gcl_%d\" % i](h, edges, x, node_mask, edge_mask, edge_attr=edge_attr, node_attr=h0, n_nodes=n_nodes)\n",
    "                if self.normalize == \"batch\" or self.normalize == \"both\":\n",
    "                    h = self._modules[\"bn_%d\" % i](h, x)\n",
    "            else:\n",
    "                h, _, _ = self._modules[\"gcl_%d\" % i](h, edges, x, node_mask, edge_mask, edge_attr=edge_attr,\n",
    "                                                      node_attr=None, n_nodes=n_nodes)\n",
    "                if self.normalize == \"batch\" or self.normalize == \"both\":\n",
    "                    h = self._modules[\"bn_%d\" % i](h, x)\n",
    "\n",
    "        h = self.node_dec(h)\n",
    "        h = h * node_mask\n",
    "        h = h.view(-1, n_nodes, self.hidden_nf)\n",
    "        h = torch.sum(h, dim=1)\n",
    "        pred = self.graph_dec(h)\n",
    "        return pred.squeeze(1)\n",
    "\n",
    "\n",
    "def train(model, optimizer, lr_scheduler, epoch, loader, Property=\"homo\", charge_power=2, partition=\"train\", dtype=torch.float32, log_interval=100):\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    res = {'loss': 0, 'counter': 0, 'loss_arr':[]}\n",
    "\n",
    "    for i, data in enumerate(loader):\n",
    "        if partition == 'train':\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        batch_size, n_nodes, _ = data['positions'].size()\n",
    "        atom_positions = data['positions'].view(batch_size * n_nodes, -1).to(device, dtype)\n",
    "        atom_mask = data['atom_mask'].view(batch_size * n_nodes, -1).to(device, dtype)\n",
    "        edge_mask = data['edge_mask'].to(device, dtype)\n",
    "        one_hot = data['one_hot'].to(device, dtype)\n",
    "        charges = data['charges'].to(device, dtype)\n",
    "        nodes = qm9_utils.preprocess_input(one_hot, charges, charge_power, charge_scale, device)\n",
    "\n",
    "        nodes = nodes.view(batch_size * n_nodes, -1)\n",
    "        # nodes = torch.cat([one_hot, charges], dim=1)\n",
    "        edges = qm9_utils.get_adj_matrix(n_nodes, batch_size, device)\n",
    "        label = data[Property].to(device, dtype)\n",
    "\n",
    "        pred = model(h0=nodes, x=atom_positions, edges=edges, edge_attr=None, node_mask=atom_mask, edge_mask=edge_mask,\n",
    "                     n_nodes=n_nodes)\n",
    "\n",
    "        if partition == 'train':\n",
    "            loss = loss_l1(pred, (label - meann) / mad)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            loss = loss_l1(mad * pred + meann, label)\n",
    "\n",
    "        res['loss'] += loss.item() * batch_size\n",
    "        res['counter'] += batch_size\n",
    "        res['loss_arr'].append(loss.item())\n",
    "\n",
    "#         if i % log_interval == 0:\n",
    "#             print(\"Epoch %d \\t Iteration %d \\t loss %.4f\" % (epoch, i, sum(res['loss_arr'][-10:])/len(res['loss_arr'][-10:])))\n",
    "    return res['loss'] / res['counter']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5994d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "loss_l1 = nn.L1Loss()\n",
    "\n",
    "# Arguments\n",
    "\n",
    "batch_size = 72\n",
    "epochs = 500\n",
    "lr = 1e-3\n",
    "nf = 80 # hidden node features\n",
    "attention=1\n",
    "# n_layers = 7\n",
    "Property = 'homo'\n",
    "charge_power = 2\n",
    "dataset_paper = \"cormorant\"\n",
    "node_attr = 0\n",
    "weight_decay = 1e-16\n",
    "test_interval = 50\n",
    "\n",
    "# Data Loading\n",
    "dataloaders, charge_scale = dataset.retrieve_dataloaders(batch_size, 2)\n",
    "meann, mad = qm9_utils.compute_mean_mad(dataloaders, Property)\n",
    "\n",
    "# Train\n",
    "train_losses = {'egnn': [], 'batch': [], 'none': [], 'both': []}\n",
    "test_losses = {'egnn': [], 'batch': [], 'none': [], 'both': []}\n",
    "\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "\n",
    "for norm in ['batch']:\n",
    "    for n_layers in [3, 7, 12, 16]:\n",
    "        model = EGNN(in_node_nf=15, in_edge_nf=0, hidden_nf=nf, device=device, n_layers=n_layers, coords_weight=1.0,\n",
    "                 attention=attention, node_attr=node_attr, normalize=norm)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "\n",
    "        training_start = datetime.datetime.now()\n",
    "\n",
    "        for epoch in range(0, epochs):\n",
    "            epoch_start = datetime.datetime.now()\n",
    "\n",
    "            train_loss = train(model, optimizer, scheduler, epoch, dataloaders['train'], Property, charge_power, partition=\"train\")\n",
    "            test_loss = train(model, optimizer, scheduler, epoch, dataloaders['valid'], Property, charge_power, partition=\"test\")\n",
    "\n",
    "            train_losses[norm].append(train_loss)\n",
    "            test_losses[norm].append(test_loss)\n",
    "\n",
    "            total_time = datetime.datetime.now() - training_start\n",
    "            total_time = total_time - datetime.timedelta(microseconds=total_time.microseconds)\n",
    "\n",
    "            epoch_time = datetime.datetime.now() - epoch_start \n",
    "            epoch_time = epoch_time - datetime.timedelta(microseconds=epoch_time.microseconds)\n",
    "\n",
    "            print (\"{}: epoch {} \\t avg test loss: {} \\t epoch time: {} \\t total time: {}\".format(\n",
    "                EXP_NAME, epoch, test_loss, str(epoch_time), str(total_time))\n",
    "                  )\n",
    "\n",
    "            if epoch % test_interval == 0:\n",
    "                torch.save(model.state_dict(), 'results/egnn{}_depth{}.pth'.format(norm, n_layers))\n",
    "                with open('results/egnn{}_depth{}_trainloss.pickle'.format(norm, n_layers), 'wb') as handle:\n",
    "                    pickle.dump(train_losses, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                with open('results/egnn{}_depth{}_testloss.pickle'.format(norm, n_layers), 'wb') as handle:\n",
    "                    pickle.dump(test_losses, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e68cf3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
